{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %pylab inline\n",
    "#start a timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#import operating system \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#To Show Images\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterplot import ProgressPlot\n",
    "\n",
    "# Import Pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchsummary import summary\n",
    "\n",
    "#Monai (Import data and transform data)\n",
    "from monai.transforms import \\\n",
    "    Compose, AddChannel, ScaleIntensity, ToTensor, Resize, RandRotate, RandFlip, RandScaleIntensity, RandZoom, RandGaussianNoise, RandAffine, ResizeWithPadOrCrop\n",
    "from monai.data import CacheDataset, ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose architecture\n",
    "α-WGANSigmaRat1 => arch1\n",
    "\n",
    "α-WGANSigmaRat2 => arch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture = \"arch1\"\n",
    "architecture = \"arch2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Generator, Discriminator, Encoder and Code Discriminator \n",
    "if architecture == \"arch1\":\n",
    "    from WGAN_SigmaRat1 import *\n",
    "if architecture == \"arch2\":\n",
    "    from WGAN_SigmaRat2 import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration -> Constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_______________________________________________Constants_______________________________________________\n",
    "DEBUG=True\n",
    "PATH_DATASET = 'path/to/the/dataset/MRI/'\n",
    "\n",
    "BEGINNING_TRAIN=False # always false, unless it is to train from the beginning\n",
    "\n",
    "#Neural net\n",
    "BATCH_SIZE = 4 # batch_size must be a divisor of the data set number.\n",
    "WORKERS = 2\n",
    "_EPS = 1e-15 #for calc_gradient_penalty\n",
    "TOTAL_ITER = 200000 \n",
    "\n",
    "#Train_Weights\n",
    "if architecture == \"arch1\":\n",
    "    #loss1 = l1_loss*L1_WEIGHT - cd_z_hat_loss*CD_Z_HAT_WEIGHT - d_loss*D_WEIGHT+ mse_loss*MSE_WEIGHT\n",
    "    L1_WEIGHT=10\n",
    "    CD_Z_HAT_WEIGHT=1\n",
    "    D_WEIGHT=1\n",
    "    MSE_WEIGHT=10\n",
    "    #----------------------\n",
    "    #loss2 = x_loss2*X_LOSS2_WEIGHT + (gradient_penalty_r+gradient_penalty_h)*GP_D_WEIGHT\n",
    "    X_LOSS2_WEIGHT=1\n",
    "    GP_D_WEIGHT=10\n",
    "    #----------------------\n",
    "    #loss3 = x_loss3*X_LOSS3_WEIGHT + gradient_penalty_cd * GP_CD_WEIGHT\n",
    "    X_LOSS3_WEIGHT=1\n",
    "    GP_CD_WEIGHT=10\n",
    "if architecture == \"arch2\":\n",
    "    #loss1 = -cd_z_hat_loss*CD_Z_HAT_WEIGHT - d_loss*D_WEIGHT + mse_loss*MSE_WEIGHT + gd_loss*GD_WEIGHT\n",
    "    CD_Z_HAT_WEIGHT=1\n",
    "    D_WEIGHT=1\n",
    "    MSE_WEIGHT=100 \n",
    "    GD_WEIGHT=1/100\n",
    "    #----------------------\n",
    "    #loss2 = x_loss2*X_LOSS2_WEIGHT + (gradient_penalty_r+gradient_penalty_h)*GP_D_WEIGHT\n",
    "    X_LOSS2_WEIGHT=1\n",
    "    GP_D_WEIGHT=100\n",
    "    #----------------------\n",
    "    #loss3 = x_loss3*X_LOSS3_WEIGHT + gradient_penalty_cd * GP_CD_WEIGHT\n",
    "    X_LOSS3_WEIGHT=1\n",
    "    GP_CD_WEIGHT=100\n",
    "#----------------------\n",
    "#setting latent variable sizes\n",
    "LATENT_DIM = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset_list(path_dataset):\n",
    "    dataset = [f for f in listdir(path_dataset) if isfile(join(path_dataset, f))]\n",
    "    train_dataset=list()\n",
    "    for i in dataset:\n",
    "        train_dataset.append(path_dataset+i)\n",
    "    return train_dataset\n",
    "\n",
    "train_dataset = create_train_dataset_list(PATH_DATASET) #creat a list of paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Monai Transformation\n",
    "* Resize (64x64x64)\n",
    "* Rotate (3º)\n",
    "* Flip (x axis)\n",
    "* Rand Scale Intensity (±0.1)\n",
    "* Zoom (1.1)\n",
    "* Gaussian Noise (0.01)\n",
    "* Scale Intensity Norm (-1,1)\n",
    "* Translate (4x4x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_loader():\n",
    "    train_transforms = Compose([AddChannel(),\n",
    "                                ResizeWithPadOrCrop(spatial_size =(64, 64, 64), mode='constant'),\n",
    "                                RandRotate(prob=0.1,range_x=0.052,range_y =0.052,range_z =0.052),\n",
    "                                RandFlip(prob=0.1,spatial_axis=0),\n",
    "                                RandScaleIntensity(prob=0.1,factors=(0.1)),\n",
    "                                RandZoom(prob=0.1,min_zoom =(1.0),max_zoom=(1.1),mode=\"nearest\"),\n",
    "                                RandGaussianNoise(prob=0.1,mean=0, std=0.01),\n",
    "                                ScaleIntensity(minv=-1.0, maxv=1.0),\n",
    "                                RandAffine(prob=0.1,translate_range=(4,4,0)),\n",
    "                                ToTensor()])\n",
    "\n",
    "    train_ds = ImageDataset(image_files=train_dataset, transform=train_transforms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    return(train_loader)\n",
    "\n",
    "train_loader=create_train_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALZIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rescale_array(arr: np.ndarray, minv: float = 0.0, maxv: float = 1.0): #monmai function adapted\n",
    "    \"\"\"\n",
    "    Rescale the values of numpy array `arr` to be from `minv` to `maxv`.\n",
    "    \"\"\"\n",
    "\n",
    "    mina = torch.min(arr)\n",
    "    maxa = torch.max(arr)\n",
    "\n",
    "    if mina == maxa:\n",
    "        return arr * minv\n",
    "\n",
    "    norm = (arr - mina) / (maxa - mina)  # normalize the array first\n",
    "    return (norm * (maxv - minv)) + minv  # rescale by minv and maxv, which is the normalized array by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks \n",
    "----\n",
    "### Nomenclature \n",
    "* G -> Generator\n",
    "* CD -> Code Discriminator\n",
    "* D -> Discriminator\n",
    "* E -> Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nets():\n",
    "    G = Generator(noise = LATENT_DIM)\n",
    "    CD = Code_Discriminator(code_size = LATENT_DIM ,num_units = 4096)\n",
    "    D = Discriminator(is_dis=True)\n",
    "    \n",
    "    # __TO_GPU___\n",
    "    G.cuda()   #|\n",
    "    CD.cuda()  #|\n",
    "    D.cuda()   #|\n",
    "    #__________#|\n",
    "    \n",
    "    \n",
    "    if architecture == \"arch1\":\n",
    "        E = Discriminator(out_class = LATENT_DIM,is_dis=False)\n",
    "        E.cuda()\n",
    "        #______________________OPTIMIZERS______________________\n",
    "        g_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
    "        d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "        e_optimizer = optim.Adam(E.parameters(), lr = 0.0002)\n",
    "        cd_optimizer = optim.Adam(CD.parameters(), lr = 0.0002)\n",
    "\n",
    "    if architecture == \"arch2\":\n",
    "        E = Encoder(out_class = LATENT_DIM,is_dis=False)\n",
    "        E.cuda()\n",
    "        #______________________OPTIMIZERS______________________torch.optim.AdamW Try this optimizer (instead of Adam).\n",
    "        g_optimizer = optim.AdamW(G.parameters(), lr=0.0002)\n",
    "        d_optimizer = optim.AdamW(D.parameters(), lr=0.0002)\n",
    "        e_optimizer = optim.AdamW(E.parameters(), lr = 0.0002)\n",
    "        cd_optimizer = optim.AdamW(CD.parameters(), lr = 0.0002)\n",
    "    \n",
    "    return (G,CD,D,E,g_optimizer,cd_optimizer,d_optimizer,e_optimizer)\n",
    "G,CD,D,E,g_optimizer,cd_optimizer,d_optimizer,e_optimizer=nets()\n",
    "if DEBUG:\n",
    "    print(\"----------------------------------------------------Networks----------------------------------------------------\")\n",
    "    print(\"--------------------------Generator--------------------------\")\n",
    "    print(G)\n",
    "    print(\"--------------------------Discriminator--------------------------\")\n",
    "    print(D)\n",
    "    print(\"--------------------------Encoder--------------------------\")\n",
    "    print(E)\n",
    "    print(\"--------------------------Code-Discriminator--------------------------\")\n",
    "    print(CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Penalty and Gradient Difference Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___________________________________________WGAN-GP gradient penalty___________________________________________\n",
    "\n",
    "#calc_gradient_penalty(model, real_data, generated_data)\n",
    "def calc_gradient_penalty(model, x, x_gen):\n",
    "    assert x.size()==x_gen.size(), \"real and sampled sizes do not match\"\n",
    "    alpha_size = tuple((len(x), *(1,)*(x.dim()-1)))\n",
    "    alpha_t = torch.cuda.FloatTensor if x.is_cuda else torch.Tensor\n",
    "    alpha = alpha_t(*alpha_size).uniform_()\n",
    "    x_hat = x.data*alpha + x_gen.data*(1-alpha)\n",
    "    x_hat = Variable(x_hat, requires_grad=True)\n",
    "\n",
    "    def eps_norm(x):\n",
    "        x = x.view(len(x), -1)\n",
    "        return (x*x+_EPS).sum(-1).sqrt()\n",
    "    def bi_penalty(x):\n",
    "        return (x-1)**2\n",
    "\n",
    "    grad_xhat = torch.autograd.grad(model(x_hat).sum(), x_hat, create_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    penalty = bi_penalty(eps_norm(grad_xhat)).mean()\n",
    "    return penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________________Gradient difference loss function_____________________\n",
    "def gdloss(real,fake):\n",
    "    '''\n",
    "    https://github.com/Y-P-Zhang/3D-GANs-pytorch/blob/master/models/losses.py\n",
    "    '''\n",
    "    dx_real = real[:, :, :, 1:, :] - real[:, :, :, :-1, :] \n",
    "    dy_real = real[:, :, 1:, :, :] - real[:, :, :-1, :, :]\n",
    "    dz_real = real[:, :, :, :, 1:] - real[:, :, :, :, :-1]\n",
    "    dx_fake = fake[:, :, :, 1:, :] - fake[:, :, :, :-1, :]\n",
    "    dy_fake = fake[:, :, 1:, :, :] - fake[:, :, :-1, :, :]\n",
    "    dz_fake = fake[:, :, :, :, 1:] - fake[:, :, :, :, :-1]\n",
    "    gd_loss = torch.sum(torch.pow(torch.abs(dx_real) - torch.abs(dx_fake),2),dim=(2,3,4)) + \\\n",
    "              torch.sum(torch.pow(torch.abs(dy_real) - torch.abs(dy_fake),2),dim=(2,3,4)) + \\\n",
    "              torch.sum(torch.pow(torch.abs(dz_real) - torch.abs(dz_fake),2),dim=(2,3,4))\n",
    "    # torch.pow(value,2)==(value)**\n",
    "    \n",
    "    return torch.sum(gd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networks_params(D, Dtruth, CD, CDtruth, E, Etruth, G, Gtruth):\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = Dtruth\n",
    "    for p in CD.parameters():  \n",
    "        p.requires_grad = CDtruth\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = Etruth\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = Gtruth\n",
    "        \n",
    "    return(D, CD, E, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_information(iteration,loss2,loss1,loss3,l1_loss,mse_loss):\n",
    "    #Save losses in lists\n",
    "    Lloss1.append(float(loss1.data.cpu().numpy()))\n",
    "    Lloss2.append(float(loss2.data.cpu().numpy()))\n",
    "    Lloss3.append(float(loss3.data.cpu().numpy()))\n",
    "    L1LossL.append(float(l1_loss.data.cpu().numpy()))\n",
    "    MSELossL.append(float(mse_loss.data.cpu().numpy()))\n",
    "    \n",
    "    print('[{}/{}]'.format(iteration,TOTAL_ITER),\n",
    "                  f'D: {loss2.data.cpu().numpy():<8.3}', \n",
    "                  f'En_Ge: {loss1.data.cpu().numpy():<8.3}',\n",
    "                  f'Code: {loss3.data.cpu().numpy():<8.3}',\n",
    "                  f'L1_Loss: {l1_loss.data.cpu().numpy():<8.3}',\n",
    "                  f'MSE_Loss: {mse_loss.data.cpu().numpy():<8.3}',\n",
    "                  )\n",
    "    return(Lloss1,Lloss2,Lloss3,L1LossL,MSELossL)\n",
    "\n",
    "def see_information2(iteration,loss2,loss1,loss3,mse_loss,gd_loss):\n",
    "    #Save losses in lists\n",
    "    Lloss1.append(float(loss1.data.cpu().numpy()))\n",
    "    Lloss2.append(float(loss2.data.cpu().numpy()))\n",
    "    Lloss3.append(float(loss3.data.cpu().numpy()))\n",
    "    MSELossL.append(float(mse_loss.data.cpu().numpy()))\n",
    "    Gd_LossL.append(float(gd_loss))\n",
    "    \n",
    "    print('[{}/{}]'.format(iteration,TOTAL_ITER),\n",
    "                  f'D: {loss2.data.cpu().numpy():<8.3}', \n",
    "                  f'En_Ge: {loss1.data.cpu().numpy():<8.3}',\n",
    "                  f'Code: {loss3.data.cpu().numpy():<8.3}',\n",
    "                  f'MSE_Loss: {mse_loss.data.cpu().numpy():<8.3}',\n",
    "                  f'Gd_Loss: {gd_loss:<8.3}',\n",
    "                  )\n",
    "    \n",
    "    return(Lloss1,Lloss2,Lloss3,MSELossL,Gd_LossL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(image,reality):\n",
    "        feat = np.squeeze((0.5*image[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(feat,title=reality)\n",
    "        plotting.show()\n",
    "        print(torch.max(image[0]))\n",
    "        print(torch.min(image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(start):\n",
    "    G.load_state_dict(torch.load('./checkpoint/G_iter'+str(start)+'.pth'))\n",
    "    CD.load_state_dict(torch.load('./checkpoint/CD_iter'+str(start)+'.pth'))\n",
    "    E.load_state_dict(torch.load('./checkpoint/E_iter'+str(start)+'.pth'))\n",
    "    D.load_state_dict(torch.load('./checkpoint/D_iter'+str(start)+'.pth'))\n",
    "    \n",
    "    g_optimizer = optim.AdamW(G.parameters(), lr=0.0002)\n",
    "    d_optimizer = optim.AdamW(D.parameters(), lr=0.0002)\n",
    "    e_optimizer = optim.AdamW(E.parameters(), lr = 0.0002)\n",
    "    cd_optimizer = optim.AdamW(CD.parameters(), lr = 0.0002)\n",
    "    \n",
    "    # usar ADAMW\n",
    "    return(G,CD,D,E,g_optimizer,cd_optimizer,d_optimizer,e_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if architecture == \"arch1\":\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_mse = nn.MSELoss()\n",
    "\n",
    "    Lloss1=list()\n",
    "    Lloss2=list()\n",
    "    Lloss3=list()\n",
    "    L1LossL=list()\n",
    "    MSELossL=list()\n",
    "    \n",
    "if architecture == \"arch2\":\n",
    "    criterion_mse = nn.MSELoss()\n",
    "\n",
    "    Lloss1=list()\n",
    "    Lloss2=list()\n",
    "    Lloss3=list()\n",
    "    MSELossL=list()\n",
    "    Gd_LossL=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(resume,start):\n",
    "    \n",
    "    pp = ProgressPlot(plot_names=[\"Generator-Encoder\", \"Discriminator\",\"Code-Discriminator\", \"Discriminator-Capacity\"],\n",
    "                 line_names=[\"Loss\"])\n",
    "    g_iter = 1\n",
    "    d_iter = 1\n",
    "    cd_iter =1\n",
    "    if resume:\n",
    "        G,CD,D,E,g_optimizer,cd_optimizer,d_optimizer,e_optimizer=load_dict(start)\n",
    "    else:\n",
    "        G,CD,D,E,g_optimizer,cd_optimizer,d_optimizer,e_optimizer=nets()\n",
    "\n",
    "    for iteration in range(start,TOTAL_ITER):\n",
    "\n",
    "        ###############################################\n",
    "        # Train Encoder - Generator \n",
    "        ###############################################\n",
    "        D, CD, E, G = networks_params(D=D, Dtruth=False, CD=CD, CDtruth=False, E=E, Etruth=True, G=G, Gtruth=True)\n",
    "        \n",
    "        for iters in range(g_iter):\n",
    "                G.zero_grad()\n",
    "                E.zero_grad()\n",
    "                real_images = next(iter(train_loader)) #next image\n",
    "                _batch_size = real_images.size(0)\n",
    "                \n",
    "                real_images = Variable(real_images,volatile=True).cuda(non_blocking =True) #next image into de GPU\n",
    "                z_rand = Variable(torch.randn((_batch_size,LATENT_DIM)),volatile=True).cuda() #random vector\n",
    "                z_hat = E(real_images).view(_batch_size,-1) #Output vector of the Encoder\n",
    "                x_hat = G(z_hat) #Generation of an image using the encoder's output vector\n",
    "                x_rand = G(z_rand) #Generation of an image using a random vector\n",
    "                \n",
    "                #Code discriminator absolute value of the encoder's output vector \n",
    "                cd_z_hat_loss = CD(z_hat).mean() #(considering the random vector as real) \n",
    "                \n",
    "                #calculation of the discriminator loss\n",
    "                d_real_loss = D(x_hat).mean() #of the output vector of the Encoder\n",
    "                d_fake_loss = D(x_rand).mean() #of the random vector\n",
    "                d_loss = d_fake_loss+d_real_loss\n",
    "                \n",
    "                if architecture == \"arch1\":\n",
    "                    #_____________MSE_loss_____________\n",
    "                    mse_loss=criterion_mse(x_hat,real_images) \n",
    "                    #_____________L1_Loss_____________\n",
    "                    l1_loss =criterion_l1(x_hat,real_images)\n",
    "\n",
    "                    #__________Generator_Loss__________\n",
    "                    loss1 = l1_loss*L1_WEIGHT - cd_z_hat_loss*CD_Z_HAT_WEIGHT - d_loss*D_WEIGHT+ mse_loss*MSE_WEIGHT\n",
    "\n",
    "                \n",
    "                if architecture == \"arch2\":\n",
    "                    #_____________Mean_Squared_Error_____________\n",
    "                    mse_loss=criterion_mse(x_hat,real_images) \n",
    "\n",
    "                    #_____________Gradient_Different_Loss_____________\n",
    "                    gd_loss=gdloss(real_images,x_hat).item() #considering axis -> x,y,z\n",
    "\n",
    "                    #__________Generator_Loss__________\n",
    "\n",
    "                    # this loss function is based on this\n",
    "                    loss1 = -cd_z_hat_loss*CD_Z_HAT_WEIGHT - d_loss*D_WEIGHT + mse_loss*MSE_WEIGHT + gd_loss*GD_WEIGHT #correct\n",
    "\n",
    "                if iters<g_iter-1:\n",
    "                    loss1.backward()\n",
    "                else:\n",
    "                    loss1.backward(retain_graph=True)\n",
    "                e_optimizer.step()\n",
    "                g_optimizer.step()\n",
    "                g_optimizer.step()\n",
    "\n",
    "        ###############################################\n",
    "        # Train Discriminator\n",
    "        ###############################################\n",
    "        D, CD, E, G = networks_params(D=D, Dtruth=True, CD=CD, CDtruth=False, E=E, Etruth=False, G=G, Gtruth=False)\n",
    "        \n",
    "        for iters in range(d_iter):\n",
    "            d_optimizer.zero_grad()\n",
    "            real_images = next(iter(train_loader)) #next image\n",
    "            _batch_size = real_images.size(0)\n",
    "             \n",
    "            z_rand = Variable(torch.randn((_batch_size,LATENT_DIM)),volatile=True).cuda() #random vector\n",
    "            real_images = Variable(real_images,volatile=True).cuda(non_blocking =True) #next image into de GPU\n",
    "            z_hat = E(real_images).view(_batch_size,-1) #Output vector of the Encoder\n",
    "            x_hat = G(z_hat) #Generation of an image using the encoder's output vector\n",
    "            x_rand = G(z_rand) #Generation of an image using a random vector\n",
    "            \n",
    "            #calculation of the discriminator loss (if it can distinguish between real and fake)\n",
    "            x_loss2 = -2*D(real_images).mean()+D(x_hat).mean()+D(x_rand).mean() \n",
    "            \n",
    "            #calculation of the gradient penalty\n",
    "            gradient_penalty_r = calc_gradient_penalty(D,real_images.data, x_rand.data)\n",
    "            gradient_penalty_h = calc_gradient_penalty(D,real_images.data, x_hat.data)\n",
    "            \n",
    "            #__________Discriminator_loss__________\n",
    "            loss2 = x_loss2*X_LOSS2_WEIGHT + (gradient_penalty_r+gradient_penalty_h)*GP_D_WEIGHT\n",
    "            loss2.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "\n",
    "        ###############################################\n",
    "        # Train Code Discriminator\n",
    "        ###############################################\n",
    "        D, CD, E, G = networks_params(D=D, Dtruth=False, CD=CD, CDtruth=True, E=E, Etruth=False, G=G, Gtruth=False)    \n",
    "        \n",
    "        for iters in range(cd_iter):\n",
    "            cd_optimizer.zero_grad()\n",
    "            #random vector (considered as real here)\n",
    "            z_rand = Variable(torch.randn((_batch_size,LATENT_DIM)),volatile=True).cuda() \n",
    "            #Gradient Penalty between randon vector and encoder's output vector\n",
    "            gradient_penalty_cd = calc_gradient_penalty(CD,z_hat.data, z_rand.data) \n",
    "            \n",
    "            x_loss3=-CD(z_rand).mean() + CD(z_hat).mean()\n",
    "            \n",
    "            #___________Code_Discriminator_Loss___________\n",
    "            loss3 = x_loss3*X_LOSS3_WEIGHT + gradient_penalty_cd * GP_CD_WEIGHT\n",
    "            loss3.backward(retain_graph=True)\n",
    "            cd_optimizer.step()\n",
    "\n",
    "        ###############################################\n",
    "        # Visualization\n",
    "        ###############################################\n",
    "        if iteration % 1000 == 0:\n",
    "            pp.update([[float(loss1.data.cpu().numpy())],\n",
    "                       [float(loss2.data.cpu().numpy())],\n",
    "                       [float(loss3.data.cpu().numpy())],\n",
    "                       [float(d_fake_loss.data.cpu().numpy())]])\n",
    "\n",
    "            \n",
    "            #Save losses in lists and print instant loss values\n",
    "            if architecture == \"arch1\":\n",
    "                Lloss1,Lloss2,Lloss3,L1LossL,MSELossL=see_information(iteration,loss2,loss1,loss3,l1_loss,mse_loss)\n",
    "            \n",
    "            if architecture == \"arch2\":\n",
    "                Lloss1,Lloss2,Lloss3,MSELossL,Gd_LossL=see_information2(iteration,loss2,loss1,loss3,mse_loss,gd_loss)\n",
    "            \n",
    "            #Show the real image\n",
    "            visualization(real_images,\"Real\")\n",
    "            #Show the generated image using encoder's output vector\n",
    "            visualization(x_hat,\"x_hat\")\n",
    "            #Show the generated image using a random vector\n",
    "            visualization(x_rand,\"x_rand\")  \n",
    "   \n",
    "        ###############################################\n",
    "        # Model Save\n",
    "        ###############################################\n",
    "        if (iteration+1)%1000 ==0:\n",
    "            torch.save(G.state_dict(),'./checkpoint/G_iter'+str(iteration+1)+'.pth')\n",
    "            torch.save(D.state_dict(),'./checkpoint/D_iter'+str(iteration+1)+'.pth')\n",
    "            torch.save(E.state_dict(),'./checkpoint/E_iter'+str(iteration+1)+'.pth')\n",
    "            torch.save(CD.state_dict(),'./checkpoint/CD_iter'+str(iteration+1)+'.pth')\n",
    "\n",
    "        resume = True\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(resume=False,start=0)\n",
    "pp.finalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
